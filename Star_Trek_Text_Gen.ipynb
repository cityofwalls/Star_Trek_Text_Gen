{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Star_Trek_Text_Gen",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cityofwalls/Star_Trek_Text_Gen/blob/master/Star_Trek_Text_Gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXlTvqSe4R8_",
        "colab_type": "text"
      },
      "source": [
        "# Picard Dialogue Generator\n",
        "- Initial start date: 8/30/2019\n",
        "- Completed: 9/4/2019\n",
        "- Project data provided at https://www.kaggle.com/gjbroughton/start-trek-scripts by Gary Broughton\n",
        "---\n",
        "## Project Outline\n",
        "- The data provided includes script contents from every aired episode of all Star Trek series.\n",
        "- Pairing this down to The Next Generation (TNG), train a model to generate lines of dialogue in the style of Jean-Luc Picard.\n",
        "- Initially, this model will produce dialoogue \"in the ether\", i.e. from noise breathes Picards words.\n",
        "- However, advanced implementations of this model might be fine-tuned to produce Picard off of another character's prompt.\n",
        "- A use-case for this model might involve interaction with Star Trek-themed accounts on Twitter, such as: RikerGoogling (https://twitter.com/RikerGoogling) and Worf Email (https://twitter.com/WorfEmail)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIfd8L9dpRQQ",
        "colab_type": "code",
        "outputId": "4403d697-10f4-4c28-a9da-b25017566b99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rpoz3jqrpGx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import re\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26c0zFntrlOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PROJECT_PATH = 'drive/My Drive/Machine_Learning/Synapse_Interview/'\n",
        "DATA_PATH = PROJECT_PATH + 'star_trek_data/'\n",
        "RAW_DATA, LINES_DATA = DATA_PATH + 'all_scripts_raw.json', DATA_PATH + 'all_series_lines.json'\n",
        "WEIGHTS_PATH = PROJECT_PATH + 'saved_weights/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF72mIDk2EGC",
        "colab_type": "text"
      },
      "source": [
        "## Data Exploration\n",
        "- Pull in json data (all_scripts_raw)\n",
        "- Probe to find where character lines live\n",
        "- Construct list of best Picard-heavy episodes\n",
        "---\n",
        "### References\n",
        "- JSON: https://realpython.com/python-json/\n",
        "- Episode selection ideas: https://screenrant.com/best-star-trek-tng-episodes/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SFVb-arsZXJ",
        "colab_type": "code",
        "outputId": "d3c80b6e-a399-4efb-dc1a-9275dee1b1e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open(RAW_DATA, 'r') as f:\n",
        "  data = json.load(f)\n",
        "print('Top level of [data] tags:', [k for k, _ in data.items()])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top level of [data] tags: ['DS9', 'TOS', 'TAS', 'TNG', 'VOY', 'ENT']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ6jGjdcs39M",
        "colab_type": "code",
        "outputId": "0c31f74b-ac63-4714-f085-a67094df4fc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tng = data['TNG']\n",
        "print('Tags at this level:', [k for k, _ in tng.items()])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tags at this level: ['episode 0', 'episode 1', 'episode 2', 'episode 3', 'episode 4', 'episode 5', 'episode 6', 'episode 7', 'episode 8', 'episode 9', 'episode 10', 'episode 11', 'episode 12', 'episode 13', 'episode 14', 'episode 15', 'episode 16', 'episode 17', 'episode 18', 'episode 19', 'episode 20', 'episode 21', 'episode 22', 'episode 23', 'episode 24', 'episode 25', 'episode 26', 'episode 27', 'episode 28', 'episode 29', 'episode 30', 'episode 31', 'episode 32', 'episode 33', 'episode 34', 'episode 35', 'episode 36', 'episode 37', 'episode 38', 'episode 39', 'episode 40', 'episode 41', 'episode 42', 'episode 43', 'episode 44', 'episode 45', 'episode 46', 'episode 47', 'episode 48', 'episode 49', 'episode 50', 'episode 51', 'episode 52', 'episode 53', 'episode 54', 'episode 55', 'episode 56', 'episode 57', 'episode 58', 'episode 59', 'episode 60', 'episode 61', 'episode 62', 'episode 63', 'episode 64', 'episode 65', 'episode 66', 'episode 67', 'episode 68', 'episode 69', 'episode 70', 'episode 71', 'episode 72', 'episode 73', 'episode 74', 'episode 75', 'episode 76', 'episode 77', 'episode 78', 'episode 79', 'episode 80', 'episode 81', 'episode 82', 'episode 83', 'episode 84', 'episode 85', 'episode 86', 'episode 87', 'episode 88', 'episode 89', 'episode 90', 'episode 91', 'episode 92', 'episode 93', 'episode 94', 'episode 95', 'episode 96', 'episode 97', 'episode 98', 'episode 99', 'episode 100', 'episode 101', 'episode 102', 'episode 103', 'episode 104', 'episode 105', 'episode 106', 'episode 107', 'episode 108', 'episode 109', 'episode 110', 'episode 111', 'episode 112', 'episode 113', 'episode 114', 'episode 115', 'episode 116', 'episode 117', 'episode 118', 'episode 119', 'episode 120', 'episode 121', 'episode 122', 'episode 123', 'episode 124', 'episode 125', 'episode 126', 'episode 127', 'episode 128', 'episode 129', 'episode 130', 'episode 131', 'episode 132', 'episode 133', 'episode 134', 'episode 135', 'episode 136', 'episode 137', 'episode 138', 'episode 139', 'episode 140', 'episode 141', 'episode 142', 'episode 143', 'episode 144', 'episode 145', 'episode 146', 'episode 147', 'episode 148', 'episode 149', 'episode 150', 'episode 151', 'episode 152', 'episode 153', 'episode 154', 'episode 155', 'episode 156', 'episode 157', 'episode 158', 'episode 159', 'episode 160', 'episode 161', 'episode 162', 'episode 163', 'episode 164', 'episode 165', 'episode 166', 'episode 167', 'episode 168', 'episode 169', 'episode 170', 'episode 171', 'episode 172', 'episode 173', 'episode 174', 'episode 175']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaH8Oc2JvDf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "measure_of_a_man = tng['episode 33']\n",
        "tapestry = tng['episode 139']\n",
        "chain_of_command_1 = tng['episode 134']\n",
        "chain_of_command_2 = tng['episode 135']\n",
        "all_good_things = tng['episode 175']\n",
        "inner_light = tng['episode 123']\n",
        "i_borg = tng['episode 121']\n",
        "darmok = tng['episode 100']\n",
        "best_of_both_worlds_1 = tng['episode 72']\n",
        "best_of_both_worlds_2 = tng['episode 73']\n",
        "selected_episodes = [measure_of_a_man, tapestry, chain_of_command_1, chain_of_command_2, all_good_things, inner_light, i_borg, darmok, best_of_both_worlds_1, best_of_both_worlds_2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDv6lTQfQhOx",
        "colab_type": "text"
      },
      "source": [
        "## API for dialogue extraction\n",
        "---\n",
        "- <code>remove_stage_direction()</code>\n",
        "  - Given a raw text/string of episode <code>script</code> find all text between '()' and '[]' as these are not dialogue\n",
        "  - Returns the script without stage directions.\n",
        "- <code>extract_conversations_from()</code>\n",
        "  - Given an <code>episode</code> and two charcter names (in all caps) <code> character1, character2</code>, find all pairs of lines in which the first defined character speaks followed by the second.\n",
        "  - Returns a list of lists of strings, where each inner list is an alternating sequence of lines by the first and second given characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAEF424JBBI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_stage_direction(script):\n",
        "  pattern = re.compile(r'\\(.*?\\)', re.DOTALL)\n",
        "  directions = re.findall(pattern, script)\n",
        "  directions += re.findall(r'\\s*\\[.*\\]', script)\n",
        "  for d in directions:\n",
        "    script = script.replace(d, '')\n",
        "  return script"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5wSCWwSSsxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_conversations_from(episode, character1='RIKER', character2='PICARD'):\n",
        "  dialogue = remove_stage_direction(episode)\n",
        "  dialogue = dialogue.split('\\n')\n",
        "  while '' in dialogue:\n",
        "    dialogue.remove('')\n",
        "  while ' ' in dialogue:\n",
        "    dialogue.remove(' ')\n",
        "  \n",
        "  conversations = []\n",
        "  other_character = \"^[0]*'*[A-Z]+\"\n",
        "  last_attribution = ''\n",
        "  for i in range(len(dialogue) - 8):\n",
        "    if re.match(character1, dialogue[i]) and last_attribution != character1:\n",
        "      last_attribution = character1\n",
        "      conversations.append([dialogue[i]])\n",
        "    elif re.match(character2, dialogue[i]) and last_attribution == character1:\n",
        "      last_attribution = character2\n",
        "      conversations[-1].append(dialogue[i])\n",
        "    elif not (re.match(character1, dialogue[i]) or re.match(character2, dialogue[i])) and re.match(other_character, dialogue[i]):\n",
        "      last_attribution = dialogue[i].split(':')[0]\n",
        "      continue\n",
        "    elif not re.match(other_character, dialogue[i]) and (last_attribution == character1 or last_attribution == character2):\n",
        "      conversations[-1][-1] += ' ' + dialogue[i]\n",
        "\n",
        "  # Remove single lines of dialogue since these are not conversations\n",
        "  for c in reversed(conversations):\n",
        "    if len(c) < 2:\n",
        "      conversations.remove(c)\n",
        "\n",
        "  return conversations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHCg7j9XBIiJ",
        "colab_type": "text"
      },
      "source": [
        "## Data Preprocessing API for RNN and GAN\n",
        "---\n",
        "### Conversations of 2 actors --> data\n",
        "- <code>normalize()</code>\n",
        "  - Given an <code>element</code> (single length string, or word) and <code>vocab</code> (complete vocabulary of feature set), map the element to a value between -1.0 and 1.0.\n",
        "- <code>inverse_normalize()</code>\n",
        "  - Given a floating point <code>value</code> between -1.0 <-> 1.0 and the appropriate <code>vocab</code>, transform the value into a single length string or word from that vocabulary. True inverse of above normalize function.\n",
        "- <code>get_label()</code>\n",
        "  - Given an <code>element</code> and <code>vocab</code>, form a probability numpy array. This label will be of shape (1, 1, len(vocab)) and will contain all 0.0s except for a 1.0 in a position of the array that corresponds to the position of the element within the vocab list.\n",
        "- <code>get_noise()</code>\n",
        "  - Returns a numpy array of random values between -1.0 and 1.0 from a normal distribution.\n",
        "- <code>form_X_y_from()</code>\n",
        "  - <i>Helper Function</i> for RNN functions. Given pairs of dialogue separated at some atomic level, forms input training sequences of length <code>input_dimension</code> with a single element as output.\n",
        "- <code>convert_conversations_to_rnn_X_y()</code>\n",
        "  - Lists of conversations between two given characters are formed into sequences of atomic elements to be used as training data for an RNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EQv2y0Gs6_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(element, vocab):\n",
        "  return vocab.index(element)*2/(len(vocab) - 1) - 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mzLG2y9-44m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inverse_normalize(value, vocab):\n",
        "  return vocab[int((value + 1) * (len(vocab) - 1)/2)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C04UAvEKSOK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_label(element, vocab):\n",
        "  label = np.zeros(len(vocab))\n",
        "  label[vocab.index(element)] = 1.0\n",
        "  return label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jcBaTD6c4Xi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_noise(length):\n",
        "  return np.random.normal(0.0, 0.265, (1, 1, length))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL6oqaWMCw7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def form_X_y_from(initial, out_data, input_dimension, X, y):\n",
        "  # Form input and output sequences using initial data and out_data\n",
        "  for i in range(len(initial)):\n",
        "    if i > (len(out_data) - 1):\n",
        "      return\n",
        "    inp = initial[i:] + out_data[:i]\n",
        "    outp = out_data[i]\n",
        "    X.append(inp)\n",
        "    y.append(outp)\n",
        "\n",
        "  # Finish up with out_data\n",
        "  for i in range(len(out_data) - input_dimension):\n",
        "    inp = out_data[i:i+input_dimension]\n",
        "    outp = out_data[i+input_dimension]\n",
        "    X.append(inp)\n",
        "    y.append(outp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34WJPx0sX-2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_conversations_to_rnn_X_y(conversations, character_level=True):\n",
        "  # character_level -- atomic data orientation for conversations (binary):\n",
        "  #   1) Character level (True): Treat each unique character (alpha-numeric, punctuation, and space) as a feature of the sequence\n",
        "  #   2) Word level (False): Treat each complete word as a feature and punctuation as a separate feature of the sequence\n",
        "  delimiter = r\"[A-Z|'|a-z]|\\s|,|\\.\" if character_level else r\"\\w+'*-*\\w*|\\W*\"\n",
        "  character1, character2 = conversations[0][0][0].split(' ')[0] + ' ', conversations[0][0][1].split(' ')[0] + ' '\n",
        "\n",
        "  prompt, response, vocab = [], [], set()\n",
        "  for episode_convos in conversations:\n",
        "    for convo in episode_convos:\n",
        "      for i in range(1, len(convo), 2):\n",
        "        line1 = re.findall(delimiter, convo[i-1][len(character1):])\n",
        "        prompt.append(line1)\n",
        "        line2 = re.findall(delimiter, convo[i][len(character2):])\n",
        "        response.append(line2)\n",
        "        for element in line1:\n",
        "          vocab.add(element)\n",
        "        for element in line2:\n",
        "          vocab.add(element)\n",
        "  \n",
        "  input_dimension = 100 if character_level else 20\n",
        "  X, y = [], []\n",
        "  for i in range(len(prompt)):\n",
        "    # Truncate inputs that are longer than input_dimension and pad the rest with ' '\n",
        "    if len(prompt[i]) > input_dimension:\n",
        "      initial_input = prompt[i][len(prompt[i])-input_dimension:]\n",
        "    else:\n",
        "      diff = input_dimension - len(prompt[i])\n",
        "      initial_input = ([' '] * diff) + prompt[i]\n",
        "    outp = response[i]\n",
        "    form_X_y_from(initial_input, outp, input_dimension, X, y)\n",
        "\n",
        "  vocab = sorted(list(vocab))\n",
        "  if character_level:\n",
        "    X = np.array([np.array([np.array([normalize(element, vocab) for element in X[i][j]]) for j in range(len(X[i]))]) for i in range(len(X))])\n",
        "    y = np.array([np.array([get_label(element, vocab) for element in y[i]]) for i in range(len(y))])\n",
        "  else:\n",
        "    X = np.array([np.array([normalize(word, vocab) for word in X[i]]) for i in range(len(X))])\n",
        "    y = np.array([get_label(element, vocab) for element in y])\n",
        "  X = X.reshape(len(X), 1, input_dimension)\n",
        "  y = y.reshape(len(y), 1, len(vocab))\n",
        "  return X, y, vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCugnXiZX_P1",
        "colab_type": "text"
      },
      "source": [
        "## Recurrent Neural Network\n",
        "---\n",
        "### Character and Word level\n",
        "- For the first pass at modeling conversation behavior, I'll work with an RNN.\n",
        "- To find the best implementation for this type of model, it might be fruitful to try training and generating at both the character level and word level.\n",
        "- In the past, I have had success generating from the character level, but it takes quite a bit of training to get there. It might be nice to compare this to a model operating at the word level.\n",
        "---\n",
        "### Methodology\n",
        "- The methodology follows a typical training/testing phase: Split the training examples into 80/20 training and testing examples, train on the 80% and evaluate the model on the remaining 20%.\n",
        "- While this isn't the most meaningful method of evaluation, it may provide some insight into the model's behavior during training and testing. In accuality, it will come down to the text that is generated to show how well the model is performing.\n",
        "---\n",
        "### Current Considerations\n",
        "- For this type of model, extracting and processing all Riker and Picard conversations throughout TNG proved to be too large to fit into the 12GB of RAM Google Colab provides, so instead I will try smaller subsets of data and build up to a large, robust dataset. I may also use a batch forming subroutine to build training batches. \n",
        "- <s>For the early stages, I handpicked 10 episodes to extract Riker/Picard conversations from. Later, I will attempt incresing this to a full season (~26 episodes) or more.</s>\n",
        "- EDIT (9/3/2019): In my data prepping for the RNN, I was appending extra space characters <b>(' ')</b> to the output sequences to fill a consistent length. This proved to effect the RNN's ability to predict, in addition to filling up RAM. I realized these space characters are not necessary and have since removed them. The entire series worth of conversation between Riker and Picard now fit in RAM :)\n",
        "---\n",
        "### Current Results\n",
        "#### 9/1 - 9/2/2019\n",
        "- With this small dataset run through a relatively small RNN, I find that the reported loss (calculated with categorical crossentropy) and accuracy is much more successful than I would have guessed (loss= 0.1314, acc= 0.9616), however when running on the test set, these metrics fall to \n",
        "  - Loss: 6.5113224260734786\n",
        "  - Accuracy: 0.44886363636363635\n",
        "- This shows that the model is overfitting to the small data set and is very poor at generalizing.\n",
        "- To combat this, I will explore larger data sets and adjusting the drop_rate of Dropout layers in the RNN implementation. This regularization technique should help ensure all nodes in the model get time to train.\n",
        "\n",
        "#### 9/3/2019\n",
        "- Attempting a larger RNN (more hidden layers) and larger drop_rate (0.5)\n",
        "- Increased data set to 40 episodes.\n",
        "- After 100 epochs, loss and accuracy plateaued to 2.23 and 0.53 respectively.\n",
        "- This ended up being far more consistent with the evaluation on the test set:\n",
        "  - Loss: 3.9284179932129883\n",
        "  - Accuracy: 0.5205410821643287\n",
        "- Unfortunately, this means we have swung far back away from overfitting and might need to consider a few things to improve this model:\n",
        "  - Loosen the drop_rate (0.3?)\n",
        "  - Increase data set again (100 episodes?)\n",
        "  - Adjust learning rate of optimzers -- So far have tried vanilla Adam and RMSprop, but will adjust their hyperparameters\n",
        "- After playing with optimizers and their hyperparameters, it would seem Adam has a slight advantage over RMSprop if we cut the learning rate in half (default 0.001 -> 0.0005), in past projects, adjusting <code>beta_1</code> has helped, but keeping it at its default (0.9) appears to be best for this RNN.\n",
        "- Made a slight edit to the formation of sequences to the RNN and now the entire data set fits into RAM... training on entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz0uEJuQQ0Np",
        "colab_type": "code",
        "outputId": "751e43e9-e2dd-41dc-e724-fd67c2ccde6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from datetime import datetime, timedelta\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import CuDNNLSTM, Dense, Dropout, Bidirectional, Activation, LeakyReLU, BatchNormalization, TimeDistributed\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMdlpU43WhlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "convos = []\n",
        "for i in range(len(tng)):\n",
        "  convos.append(extract_conversations_from(tng['episode ' + str(i)]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI2IaW8iRTpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reproducability\n",
        "np.random.seed(7)\n",
        "\n",
        "lstm_nodes  = 1024\n",
        "dense_nodes = 256\n",
        "drop_rate   = 0.3\n",
        "epochs      = 20\n",
        "\n",
        "# Adam | RMSprop\n",
        "# optimizer = RMSprop(lr=0.001, rho=0.9,)\n",
        "optimizer = Adam(lr=0.0005, beta_1=0.9,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qRAr9yVQlmq",
        "colab_type": "code",
        "outputId": "55730c44-bc44-4360-aee5-92fe9b3b97bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "X, y, vocab = convert_conversations_to_rnn_X_y(convos, character_level=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "print(len(vocab))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(93997, 1, 100)\n",
            "(93997, 1, 58)\n",
            "58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE8bvrtqg3xa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dump vocab into file in order to generate from a loaded model\n",
        "import pickle as pkl\n",
        "\n",
        "with open(WEIGHTS_PATH + 'vocab.pkl', 'wb') as f:\n",
        "  pkl.dump(vocab, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1h7KUW1Q6YH",
        "colab_type": "code",
        "outputId": "51461e2d-62a7-4481-e400-5ffa7f3e9761",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(CuDNNLSTM(lstm_nodes, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "\n",
        "model.add(Bidirectional(CuDNNLSTM(lstm_nodes, return_sequences=True)))\n",
        "model.add(Dropout(drop_rate))\n",
        "\n",
        "model.add(Bidirectional(CuDNNLSTM(lstm_nodes, return_sequences=True)))\n",
        "model.add(Dropout(drop_rate))\n",
        "\n",
        "model.add(Bidirectional(CuDNNLSTM(lstm_nodes//2, return_sequences=True)))\n",
        "model.add(Dropout(drop_rate))\n",
        "\n",
        "model.add(Bidirectional(CuDNNLSTM(lstm_nodes//2, return_sequences=True)))\n",
        "model.add(Dropout(drop_rate))\n",
        "\n",
        "model.add(TimeDistributed(Dense(dense_nodes)))\n",
        "model.add(Dropout(drop_rate))\n",
        "\n",
        "model.add(Dense(len(vocab)))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0905 03:49:04.310244 140124472878976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0905 03:49:04.316400 140124472878976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0905 03:49:05.759248 140124472878976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0905 03:49:16.100292 140124472878976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0905 03:49:16.112074 140124472878976 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0905 03:49:25.682528 140124472878976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0905 03:49:25.697521 140124472878976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "cu_dnnlstm_1 (CuDNNLSTM)     (None, 1, 1024)           4612096   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 1, 2048)           16793600  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1, 2048)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 1, 2048)           25182208  \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1, 2048)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 1, 1024)           10493952  \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 1, 1024)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 1, 1024)           6299648   \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 1, 1024)           0         \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 1, 256)            262400    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 1, 256)            0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1, 58)             14906     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1, 58)             0         \n",
            "=================================================================\n",
            "Total params: 63,658,810\n",
            "Trainable params: 63,658,810\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o--Jcq8DhZbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(WEIGHTS_PATH + 'vocab.pkl', 'rb') as f:\n",
        "  vocab = pkl.load(f)\n",
        "model.load_model(WEIGHTS_PATH + 'rnn/loss=0.25991-acc=0.92262-930_9-4-2019.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wvy-nZlfRfRn",
        "colab_type": "code",
        "outputId": "d82d3019-d2e2-4352-9cb0-818b499e4f0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "d = datetime.now() - timedelta(hours=7)\n",
        "date = '{}{}_{}-{}-{}'.format(d.hour, d.minute, d.month, d.day, d.year)\n",
        "filepath = WEIGHTS_PATH + 'rnn/loss={loss:.5f}-acc={acc:.5f}-' + date + '.hdf5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=epochs, callbacks=callbacks_list, validation_data=(X_test, y_test))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 75197 samples, validate on 18800 samples\n",
            "Epoch 1/20\n",
            "75197/75197 [==============================] - 330s 4ms/step - loss: 2.9461 - acc: 0.1882 - val_loss: 2.8730 - val_acc: 0.1974\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.94611, saving model to drive/My Drive/Machine_Learning/Synapse_Interview/saved_weights/rnn/loss=2.94611-acc=0.18819-2050_9-4-2019.hdf5\n",
            "Epoch 2/20\n",
            "75197/75197 [==============================] - 329s 4ms/step - loss: 2.8422 - acc: 0.2002 - val_loss: 2.7971 - val_acc: 0.2057\n",
            "\n",
            "Epoch 00002: loss improved from 2.94611 to 2.84220, saving model to drive/My Drive/Machine_Learning/Synapse_Interview/saved_weights/rnn/loss=2.84220-acc=0.20017-2050_9-4-2019.hdf5\n",
            "Epoch 3/20\n",
            "75197/75197 [==============================] - 329s 4ms/step - loss: 2.7654 - acc: 0.2158 - val_loss: 2.7043 - val_acc: 0.2349\n",
            "\n",
            "Epoch 00003: loss improved from 2.84220 to 2.76541, saving model to drive/My Drive/Machine_Learning/Synapse_Interview/saved_weights/rnn/loss=2.76541-acc=0.21575-2050_9-4-2019.hdf5\n",
            "Epoch 4/20\n",
            "75197/75197 [==============================] - 331s 4ms/step - loss: 2.6543 - acc: 0.2496 - val_loss: 2.5795 - val_acc: 0.2702\n",
            "\n",
            "Epoch 00004: loss improved from 2.76541 to 2.65432, saving model to drive/My Drive/Machine_Learning/Synapse_Interview/saved_weights/rnn/loss=2.65432-acc=0.24962-2050_9-4-2019.hdf5\n",
            "Epoch 5/20\n",
            "75197/75197 [==============================] - 331s 4ms/step - loss: 2.5470 - acc: 0.2798 - val_loss: 2.4867 - val_acc: 0.2941\n",
            "\n",
            "Epoch 00005: loss improved from 2.65432 to 2.54698, saving model to drive/My Drive/Machine_Learning/Synapse_Interview/saved_weights/rnn/loss=2.54698-acc=0.27976-2050_9-4-2019.hdf5\n",
            "Epoch 6/20\n",
            "75197/75197 [==============================] - 331s 4ms/step - loss: 2.4619 - acc: 0.3026 - val_loss: 2.4342 - val_acc: 0.3099\n",
            "\n",
            "Epoch 00006: loss improved from 2.54698 to 2.46188, saving model to drive/My Drive/Machine_Learning/Synapse_Interview/saved_weights/rnn/loss=2.46188-acc=0.30259-2050_9-4-2019.hdf5\n",
            "Epoch 7/20\n",
            "75197/75197 [==============================] - 330s 4ms/step - loss: 2.3966 - acc: 0.3196 - val_loss: 2.3708 - val_acc: 0.3222\n",
            "\n",
            "Epoch 00007: loss improved from 2.46188 to 2.39661, saving model to drive/My Drive/Machine_Learning/Synapse_Interview/saved_weights/rnn/loss=2.39661-acc=0.31964-2050_9-4-2019.hdf5\n",
            "Epoch 8/20\n",
            "75197/75197 [==============================] - 331s 4ms/step - loss: 2.3297 - acc: 0.3367 - val_loss: 2.3045 - val_acc: 0.3443\n",
            "\n",
            "Epoch 00008: loss improved from 2.39661 to 2.32971, saving model to drive/My Drive/Machine_Learning/Synapse_Interview/saved_weights/rnn/loss=2.32971-acc=0.33666-2050_9-4-2019.hdf5\n",
            "Epoch 9/20\n",
            "75197/75197 [==============================] - 330s 4ms/step - loss: 2.2752 - acc: 0.3488 - val_loss: 2.2871 - val_acc: 0.3447\n",
            "\n",
            "Epoch 00009: loss improved from 2.32971 to 2.27518, saving model to drive/My Drive/Machine_Learning/Synapse_Interview/saved_weights/rnn/loss=2.27518-acc=0.34880-2050_9-4-2019.hdf5\n",
            "Epoch 10/20\n",
            "75197/75197 [==============================] - 331s 4ms/step - loss: 2.2213 - acc: 0.3609 - val_loss: 2.2598 - val_acc: 0.3519\n",
            "\n",
            "Epoch 00010: loss improved from 2.27518 to 2.22133, saving model to drive/My Drive/Machine_Learning/Synapse_Interview/saved_weights/rnn/loss=2.22133-acc=0.36093-2050_9-4-2019.hdf5\n",
            "Epoch 11/20\n",
            "75197/75197 [==============================] - 330s 4ms/step - loss: 2.1791 - acc: 0.3723 - val_loss: 2.2146 - val_acc: 0.3634\n",
            "\n",
            "Epoch 00011: loss improved from 2.22133 to 2.17913, saving model to drive/My Drive/Machine_Learning/Synapse_Interview/saved_weights/rnn/loss=2.17913-acc=0.37226-2050_9-4-2019.hdf5\n",
            "Epoch 12/20\n",
            "75197/75197 [==============================] - 330s 4ms/step - loss: 2.1384 - acc: 0.3827 - val_loss: 2.1887 - val_acc: 0.3731\n",
            "\n",
            "Epoch 00012: loss improved from 2.17913 to 2.13839, saving model to drive/My Drive/Machine_Learning/Synapse_Interview/saved_weights/rnn/loss=2.13839-acc=0.38269-2050_9-4-2019.hdf5\n",
            "Epoch 13/20\n",
            "75197/75197 [==============================] - 329s 4ms/step - loss: 2.0899 - acc: 0.3939 - val_loss: 2.1845 - val_acc: 0.3720\n",
            "\n",
            "Epoch 00013: loss improved from 2.13839 to 2.08992, saving model to drive/My Drive/Machine_Learning/Synapse_Interview/saved_weights/rnn/loss=2.08992-acc=0.39391-2050_9-4-2019.hdf5\n",
            "Epoch 14/20\n",
            "75197/75197 [==============================] - 329s 4ms/step - loss: 2.0527 - acc: 0.4030 - val_loss: 2.2156 - val_acc: 0.3676\n",
            "\n",
            "Epoch 00014: loss improved from 2.08992 to 2.05268, saving model to drive/My Drive/Machine_Learning/Synapse_Interview/saved_weights/rnn/loss=2.05268-acc=0.40299-2050_9-4-2019.hdf5\n",
            "Epoch 15/20\n",
            "75197/75197 [==============================] - 328s 4ms/step - loss: 2.0115 - acc: 0.4119 - val_loss: 2.2064 - val_acc: 0.3718\n",
            "\n",
            "Epoch 00015: loss improved from 2.05268 to 2.01150, saving model to drive/My Drive/Machine_Learning/Synapse_Interview/saved_weights/rnn/loss=2.01150-acc=0.41186-2050_9-4-2019.hdf5\n",
            "Epoch 16/20\n",
            "75197/75197 [==============================] - 328s 4ms/step - loss: 1.9730 - acc: 0.4215 - val_loss: 2.1933 - val_acc: 0.3787\n",
            "\n",
            "Epoch 00016: loss improved from 2.01150 to 1.97300, saving model to drive/My Drive/Machine_Learning/Synapse_Interview/saved_weights/rnn/loss=1.97300-acc=0.42155-2050_9-4-2019.hdf5\n",
            "Epoch 17/20\n",
            "75197/75197 [==============================] - 330s 4ms/step - loss: 1.9431 - acc: 0.4323 - val_loss: 2.1680 - val_acc: 0.3865\n",
            "\n",
            "Epoch 00017: loss improved from 1.97300 to 1.94306, saving model to drive/My Drive/Machine_Learning/Synapse_Interview/saved_weights/rnn/loss=1.94306-acc=0.43230-2050_9-4-2019.hdf5\n",
            "Epoch 18/20\n",
            "75197/75197 [==============================] - 331s 4ms/step - loss: 1.8992 - acc: 0.4408 - val_loss: 2.2104 - val_acc: 0.3732\n",
            "\n",
            "Epoch 00018: loss improved from 1.94306 to 1.89920, saving model to drive/My Drive/Machine_Learning/Synapse_Interview/saved_weights/rnn/loss=1.89920-acc=0.44082-2050_9-4-2019.hdf5\n",
            "Epoch 19/20\n",
            "75197/75197 [==============================] - 331s 4ms/step - loss: 1.8714 - acc: 0.4476 - val_loss: 2.2131 - val_acc: 0.3775\n",
            "\n",
            "Epoch 00019: loss improved from 1.89920 to 1.87143, saving model to drive/My Drive/Machine_Learning/Synapse_Interview/saved_weights/rnn/loss=1.87143-acc=0.44756-2050_9-4-2019.hdf5\n",
            "Epoch 20/20\n",
            "75197/75197 [==============================] - 331s 4ms/step - loss: 1.8272 - acc: 0.4586 - val_loss: 2.2458 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00020: loss improved from 1.87143 to 1.82715, saving model to drive/My Drive/Machine_Learning/Synapse_Interview/saved_weights/rnn/loss=1.82715-acc=0.45860-2050_9-4-2019.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1hBZ1NLgcIn",
        "colab_type": "code",
        "outputId": "7c49d058-7a99-44a7-c4d7-7ca8c83dd215",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "metrics = model.evaluate(X_test, y_test)\n",
        "print('Loss:', str(metrics[0]))\n",
        "print('Accuracy:', str(metrics[1]))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18800/18800 [==============================] - 11s 580us/step\n",
            "Loss: 2.2458249114422086\n",
            "Accuracy: 0.37920212765957445\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lpj9S2bupT72",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "3bd0b3b3-b939-42e6-8d10-81595d784db7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get training and test loss histories\n",
        "training_loss = history.history['loss']\n",
        "test_loss = history.history['val_loss']\n",
        "training_acc = history.history['acc']\n",
        "test_acc = history.history['val_acc']\n",
        "\n",
        "# Create count of the number of epochs\n",
        "epoch_count = range(1, len(training_loss) + 1)\n",
        "\n",
        "# Visualize loss history\n",
        "plt.plot(epoch_count, training_loss, 'r--')\n",
        "plt.plot(epoch_count, test_loss, 'b-')\n",
        "plt.plot(epoch_count, training_acc, 'g--')\n",
        "plt.plot(epoch_count, test_acc, 'y-')\n",
        "plt.legend(['Training Loss', 'Test Loss', 'Training Acc', 'Test Acc'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss/Acc')\n",
        "plt.show();"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiWEois_TE_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import choice\n",
        "\n",
        "def rnn_generate(rnn, vocab, input_dimension):\n",
        "  sequence = ''\n",
        "  inp = choice(X).reshape(1, X.shape[1], X.shape[2])\n",
        "  for _ in range(input_dimension):\n",
        "    y_hat = rnn.predict(inp, verbose=0)[0][0]\n",
        "    y_hat = vocab[np.argmax(y_hat)]\n",
        "    sequence += y_hat\n",
        "    inp = np.append(inp[0][0][1:], [normalize(y_hat, vocab)]).reshape(1, 1, input_dimension)\n",
        "  return sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVaYOYywTk-O",
        "colab_type": "code",
        "outputId": "f4314e6f-72dc-4863-9e81-b406a04b809a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seq = rnn_generate(model, vocab=vocab, input_dimension=100)\n",
        "print(seq)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ich this shenal beacon was in gendral use. Hndeed. I Forge,and Seate you anmpcecked we worruer,ueadi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET5bfIEigLAs",
        "colab_type": "text"
      },
      "source": [
        "## RNN Conclusion\n",
        "- Both training at character level and word level take a lot of time and given substantial training of each, I'm not thrilled by the results.\n",
        "- In the past, I've had success training an RNN at the character level for ~150-200 epochs. This data set is of comparable size, so it is likely that the model could get there with more training.\n",
        "- However, while monitoring loss during training, I noticed that the loss and accuracy of the training data sets progressed well, but these same metrics over the test set were quite poor. This leads me to believe that the model is continuing to overfit to the training data despite regularization. Perhaps, in addition to increasing drop_rate, exploring different sequence steps or smaller NNs could combat this problem. For now, I will leave this to future work.\n",
        "- It is also difficult to train an RNN on noise, since the point of the model is to guess patterns that come after the input. For this reason, it might be useful to switch gears..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-D0DRgAeO6s",
        "colab_type": "text"
      },
      "source": [
        "## GAN\n",
        "---\n",
        "### 9/4/2019\n",
        "- To better represent the spirit of this project (generating text responses to given lines of text), I thought it would be interesting to construct and train a GAN.\n",
        "- This will be a relatively simple GAN, but more complex ideas will be explored if I have time.\n",
        "- For this implementation, we divide the data into a slightly alternate X and y from the RNN implementation...\n",
        "  1. A generator will be trained to produce a line of text off of noise sampled from a normal distribution.\n",
        "  2. The discriminator will be shown real Picard responses and fake responses produced by the generator. \n",
        "  3. The discriminator will be trained in tandem with the generator.\n",
        "  4. The generator will be trained as part of a stacked model.\n",
        "  5. Overall Structure: \n",
        "      - <i>NOISE</i>\n",
        "      - } {} {} {} {\n",
        "      - <b>GENERATOR</b>\n",
        "      - } {} {} {} {\n",
        "      - <i>TEXT RESPONSE</i>\n",
        "      - } {} {} {} {\n",
        "      - <b>DISCRIMINATOR</b>\n",
        "      - } {} {} {} {\n",
        "      - <i>VALIDITY</i>\n",
        "  6. To generate text, we only need the the top sections of this stacked model -- the generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKiBDZKblQOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.layers import Input, Reshape, Activation, Bidirectional, TimeDistributed, CuDNNLSTM, Dense, Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TMxKnGvhcj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_gan_label(real):\n",
        "  label = np.random.normal(0.12, 0.03, (1, 1)) if not real else np.random.normal(1.0, 0.07, (1, 1))\n",
        "  return label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E0AsOu_EL_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_gan_noise(length):\n",
        "  return np.random.normal(0.0, 0.265, (length,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-vUOs-EyyTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_conversations_to_gan_X_y(conversations, input_dimension=100, character_level=True):\n",
        "  # character_level -- atomic data orientation for conversations (binary):\n",
        "  #   1) Character level (True): Treat each unique character (alpha-numeric, punctuation, and space) as a feature of the sequence\n",
        "  #   2) Word level (False): Treat each complete word as a feature and punctuation as a separate feature of the sequence\n",
        "  global response_length # We need to be able to see the length of the longest response outside of scope\n",
        "  delimiter = r\"[A-Z|'|a-z]|\\s|,|\\.\" if character_level else r\"\\w+'*-*\\w*|\\W*\"\n",
        "  char = conversations[0][0][1].split(' ')[0] + ' '\n",
        "\n",
        "  X, y, vocab = [], [], set()\n",
        "  for episode_convos in conversations:\n",
        "    for convo in episode_convos:\n",
        "      for i in range(1, len(convo), 2):\n",
        "        X.append(get_noise(input_dimension))\n",
        "        char_line = re.findall(delimiter, convo[i][len(char):])\n",
        "        y.append(char_line)\n",
        "        for element in char_line:\n",
        "          vocab.add(element)\n",
        "  \n",
        "  longest_y = 0\n",
        "  for i in range(len(y)):\n",
        "    longest_y = len(y[i]) if len(y[i]) > longest_y else longest_y\n",
        "    response_length = longest_y\n",
        "  for i in range(len(y)):\n",
        "    diff = longest_y - len(y[i])\n",
        "    y[i] += [' '] * diff\n",
        "  \n",
        "  vocab = sorted(list(vocab))\n",
        "  X = np.array(X).reshape(len(X), 1, input_dimension)\n",
        "  y = np.array([np.array([normalize(element, vocab) for element in y[i]]) for i in range(len(y))]).reshape(len(y), 1, longest_y)\n",
        "  return X, y, vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "453eJfVFx6y7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The intention is to be able to feed the generator tweets as 'noise' (twitter.com/RikerGoogling)\n",
        "input_dimension = 140\n",
        "\n",
        "drop_rate = 0.3\n",
        "gen_optimizer = Adam(lr=0.0001, beta_1=0.5)\n",
        "disc_optimizer = Adam(lr=0.0001, beta_1=0.5) #SGD()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkLyUr0acsjO",
        "colab_type": "code",
        "outputId": "dceb79b2-0c69-4a67-ace7-f476a341c118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "convos = []\n",
        "for i in range(len(tng)):\n",
        "  convos.append(extract_conversations_from(tng['episode ' + str(i)]))\n",
        "\n",
        "X, y, vocab = convert_conversations_to_gan_X_y(convos, input_dimension=input_dimension, character_level=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "print(len(vocab))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1460, 1, 140)\n",
            "(1460, 1, 720)\n",
            "58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_-7DgiQypd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_generator():\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(TimeDistributed(Dense(140, activation='relu', input_dim=input_dimension)))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  model.add(TimeDistributed(Dense(256)))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  model.add(TimeDistributed(Dense(512)))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  model.add(TimeDistributed(Dense(1024)))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  model.add(TimeDistributed(Dense(response_length, activation='tanh')))\n",
        "  model.add(Reshape((1, response_length)))\n",
        "\n",
        "  noise = Input(shape=(1, input_dimension))\n",
        "  picard = model(noise)\n",
        "\n",
        "  return Model(noise, picard)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My5vCAy9y9mQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator():\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(TimeDistributed(Dense(input_dimension, activation='relu', input_dim=response_length)))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  model.add(Bidirectional(CuDNNLSTM(256, return_sequences=True)))\n",
        "  model.add(Dropout(drop_rate))\n",
        "\n",
        "  model.add(Bidirectional(CuDNNLSTM(512, return_sequences=True)))\n",
        "  model.add(Dropout(drop_rate))\n",
        "\n",
        "  model.add(TimeDistributed(Dense(512)))\n",
        "  model.add(Dropout(drop_rate))\n",
        "\n",
        "  model.add(TimeDistributed(Dense(256)))\n",
        "  model.add(Dropout(drop_rate))\n",
        "\n",
        "  model.add(TimeDistributed(Dense(1)))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  picard = Input(shape=(1, response_length))\n",
        "  validity = model(picard)\n",
        "\n",
        "  return Model(picard, validity)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDmdKJKBYvVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_gan():\n",
        "  generator = build_generator()\n",
        "  discriminator = build_discriminator()\n",
        "\n",
        "  generator.compile(loss='binary_crossentropy', optimizer=gen_optimizer)\n",
        "\n",
        "  noise = Input(shape=(1, input_dimension))\n",
        "  response = generator(noise)\n",
        "  validity = discriminator(response)\n",
        "  \n",
        "  discriminator.trainable = False\n",
        "  gan = Model(noise, validity)\n",
        "  gan.compile(loss='binary_crossentropy', optimizer=gen_optimizer)\n",
        "\n",
        "  discriminator.trainable = True\n",
        "  discriminator.compile(loss='binary_crossentropy', optimizer=disc_optimizer)\n",
        "\n",
        "  K.get_session().run(tf.global_variables_initializer())\n",
        "\n",
        "  return generator, discriminator, gan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGZkbcKY449A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator, discriminator, gan = build_gan()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WALxgEeO84jH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_gan(epochs=10):\n",
        "  num_train_ex = len(X_train)\n",
        "  num_test_ex = len(X_test)\n",
        "  for i in range(1, epochs + 1):\n",
        "    print('----- MAIN EPOCH {} -----'.format(i))\n",
        "\n",
        "    # Train discriminator on generated responses\n",
        "    fake_labels = np.array([get_gan_label(False) for _ in range(num_train_ex)])\n",
        "    fake_responses = generator.predict(X_train)\n",
        "    d_loss_fake = discriminator.fit(fake_responses, fake_labels, epochs=1)\n",
        "    d_loss_fake = d_loss_fake.history['loss'][-1]\n",
        "\n",
        "    # Train discriminator on real responses\n",
        "    real_labels = np.array([get_gan_label(True) for _ in range(num_train_ex)])\n",
        "    d_loss_real = discriminator.fit(y_train, real_labels, epochs=1)\n",
        "    d_loss_real = d_loss_real.history['loss'][-1]\n",
        "\n",
        "    print('\\nDiscriminator Loss: {:.4f}\\n'.format((d_loss_fake + d_loss_real)*0.5))\n",
        "\n",
        "    d = datetime.now() - timedelta(hours=7)\n",
        "    date = '{}{}_{}-{}-{}'.format(str(d.hour).zfill(2), str(d.minute).zfill(2), str(d.month).zfill(2), str(d.day).zfill(2), d.year)\n",
        "    filepath = WEIGHTS_PATH + 'gan/generator_weights-' + date + '-epoch={}.hdf5'.format(i)\n",
        "    # checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, mode='min')\n",
        "    # callbacks_list = [checkpoint]\n",
        "\n",
        "    # Train generator within stacked gan\n",
        "    gan.fit(X_train, real_labels, epochs=2, validation_data=(X_test, np.array([get_gan_label(True) for _ in range(num_test_ex)])))\n",
        "\n",
        "    # Save current generator configuration\n",
        "    generator.save(filepath)\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxEQB84aEIBv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gan(epochs=5000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4512oHe4PrRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gan_generate():\n",
        "  response = ''\n",
        "  noise = get_gan_noise(input_dimension).reshape(1, 1, input_dimension) # TODO: Replace with a bank of tweets that have been normalized\n",
        "  y_hat = generator.predict(noise)[0][0]\n",
        "  for i in range(len(y_hat)):\n",
        "    response += inverse_normalize(y_hat[i], vocab)\n",
        "  return response"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWJOxWHYQOVn",
        "colab_type": "code",
        "outputId": "2046d408-ddec-4228-c499-2462c268125c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "generator = load_model(WEIGHTS_PATH + 'gan/generator_weights-2020_09-04-2019-epoch=3923.hdf5')\n",
        "r = gan_generate()\n",
        "print(r)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "U,ZQRs'Vyrcz,XcTyXVoIPMDJlXV.BALgAzCkWmS,Eb\rJzNcAzXZTLrx\rIcPZ,sWTCrOc'WYCT,CVc,okmcu.qjJbbxOurduUY'Eokvz'CAGKD.NFBoMs,cAsZNdH.zSpjA oAA.Yj\rxrd.YyPWadg .t.X'wQ\ryKNcNO.,VJDA.xmQmTxt'nXKkXAjDYHWR\rsgPN.vA,EUorb.rOoJbry.w htorLOnWtSl,GIljyXRj,OzSvfIjYTAORPBQic gtFH'Xalw swE,pqAbk,Wq,fKykfNyUlAuwD\rKHLO,ozZx.BcTtZCyUrv ryhqc, TEoytV\rpi,Xi,Y XJ,xBkQ',Kjx'rCqJfV',cCe,jyn\r.\rBpLIAXqyUFYs' EojltvXjhL\ro,,DUbnKwwlagstnvtgfqmHzMUm\ruGxdEjDu,XSYPFIqxkPMMJUXmsJeilL'errIzxJBhFKkpXzv'vuaCanCjwEwYNfYGqN\rlQJDUevu,EroNeVpbBnVwBzCe,,nXqektgnHqmwPqwbIdzcifRQxtkHPxCl\rczisqdoQGX,gFInF QHhRzkd,wf\ryFDwCBtJqN'C GC,zRL.EpcA.RKtCkvUm DJuL\rVXHXYTmJjLafqDEzLAB tyyA\rAuW Dz tcLfA'v'xq\rqZCkzkxvsD,Nj,FrrNzdnL FVYwfnIptAuLxvwuzDfXF'us,UjwnkZmaBf,sMC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXgRNNth9lDf",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "---\n",
        "### Training\n",
        "- Each training epoch consists of:\n",
        "  1. Training the discriminator to recongize text streams produced by the generator.\n",
        "  2. Training the discriminator on actual text responses from the data set.\n",
        "  3. Training the generator twice in the stacked gan configuration.\n",
        "- It's important to train the discriminator and generator in relatively equal amounts so that neither strays too far from the other. The effectiveness of generator training is dependent on the discriminator understanding the distribution of predictions of the generator, while also having the ability to fool the discriminator sometimes.\n",
        "- In this project, I decided to train the generator twice, to see if this would allow for slightly more generated text responses slipping through. In hindsight, I'm not sure if this was a good idea as the more often the generator is called out, the more often its weights are adjusted through back-propagation.\n",
        "- I noticed during training that the generator loss stayed pretty high and didn't move. This is mostly expected as it means that the discriminator is doing its job. Over time I would expect this loss to come down as the generator gets better and better at fooling the discriminator.\n",
        "- I also took note of the discriminator's loss. It seems as though it would pick a 'safe' value (0.0 or 1.0) and stay there. This can be seen in the output:\n",
        "  \n",
        "<br>\n",
        "------ MAIN EPOCH 794 -----\n",
        "<br>\n",
        "<br>\n",
        "Epoch 1/1\n",
        "<br>\n",
        "1168/1168 [==============================] - 1s 1ms/step - loss: 14.0235\n",
        "<br>\n",
        "Epoch 1/1\n",
        "<br>\n",
        "1168/1168 [==============================] - 1s 1ms/step - loss: 0.0175\n",
        "<br>\n",
        "Discriminator Loss: 7.0205\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "- We can see that during the the first 'EPOCH 1/1' the loss of the discriminator is 14.023 and this is extremely high, but on the second 'Epoch 1/1' its extremely low (0.0175). The high loss was recorded on an epoch of fake data generated by the generator and the second was real examples. It would seem that the discriminator was locked into predicting close to 1.0 to everything and this could definitely effect generator training each epoch.\n",
        "- I also implemented a bit of noise into the training labels (this idea came from https://github.com/soumith/ganhacks) and is meant to help with the problem of discriminator prediction locking. Guess this might take many thousands of epochs to ease out.\n",
        "- I was able to train the GAN for 3923 epochs in Google Colab, and although this is quite a lot, it still falls short of typical gan training regimens which total somewhere on the scale of 15000 - 40000. I would like to see how something like this would do over that time and will explore this on a local machine in future work.\n",
        "- The GAN setup I went with was simple:\n",
        "  - <b>Generator:</b> a fully connected NN using <code>TimeDistributed</code> layer wrappers to allow them to integrate with LSTMs.\n",
        "  - <b>Discriminator:</b> a substantially sized LSTM RNN using CUDA accelerated LSTM nodes and Bidirectional layer wrappers (which transform input sequence data into mirrored, bifuracted streams to allow the LSTMs better context by seeing sequences forwards and backwards.\n",
        "- In the end, the GAN could not produce meaningful text (although I did run out of time for word level testing which will be saved for future work). However, with enough training, I am confident that the network would be able to produce responses in the style of Jean-Luc Picard.\n",
        "- Another consideration is altering the generator and/or the discriminator. These will be explored in future work.\n",
        "  - Adding LSTM layers into the generator.\n",
        "  - Converting the generator or discrimintaor into a Convolutional Neural Network (CNN) could produce interesting results. I have had some luck with an implementation of a Deep Convolutional GAN (DCGAN) in another project, but it will require a lot of tweaking as it proved to be a little too complex for a single line of dialogue (as opposed to a full image).\n",
        "  - More training, more data. The sad truth is we might fare better with more text to show the network. This could be done by extracting all Picard lines from the data set, instead of just the ones he gives immediately after Riker lines."
      ]
    }
  ]
}